---
title: Translating Videos to Natural Language Using Deep Recurrent Neural Networks
date: 2023-10-21 11:52:08
tags:
---
## (reading this paper)
### Abstract
&ensp;&ensp;Solving the visual symbol grounding problem has long been a goal of artificial intelligence. The field appears to be advancing closer to this goal with recent breakthroughs in deep learning for natural language grounding in static images. In this paper, we propose to translate videos directly to sentences using a unified deep neural network with both convolutional and recurrent structure. Described video datasets are scarce, and most existing methods have been applied to toy domains with a small vocabulary of possible words. By transferring knowledge from 1.2M+ images with category labels and 100,000+ images with captions, our method is able to create sentence descriptions of open-domain videos with large vocabularies. We compare our approach with recent work using language generation metrics, subject, verb, and object prediction accuracy, and a human evaluation.

### Introduction
&ensp;&ensp;For most people, watching a brief video and describing what happened (in words) is an easy task. For machines, extracting the meaning from video pixels and generating natural-sounding language is a very complex problem. Solutions have been proposed for narrow domains with a small set of known actions and objects, e.g., (Barbu et al., 2012; Rohrbach et al., 2013), but generating descriptions for “in-thewild” videos such as the YouTube domain (Figure 1) remains an open challenge. Progress in open-domain video description has been difficult in part due to large vocabularies and very limited training data consisting of videos with associated descriptive sentences. Another serious obstacle has been the lack of rich models that can capture the joint dependencies of a sequence of frames and a corresponding sequence of words. Previous work has simplified the problem by detecting a fixed set of semantic roles, such as subject, verb, and object (Guadarrama et al., 2013; Thomason et al., 2014), as an intermediate representation. This fixed representation is problematic for large vocabularies and also leads to oversimplified rigid sentence templates which are unable to model the complex structures of natural language. In this paper, we propose to translate from video pixels to natural language with a single deep neural network. Deep NNs can learn powerful features (Donahue et al., 2013; Zeiler and Fergus, 2014), but require a lot of supervised training data. We address the problem by transferring knowledge from auxiliary tasks. Each frame of the video is modeled by a convolutional (spatially-invariant) network pre-trained on 1.2M+ images with category labels (Krizhevsky et al., 2012). The meaning state and sequence of words is modeled by a recurren (temporally invariant) deep network pre-trained on 100K+ Flickr (Hodosh and Hockenmaier, 2014) and COCO (Lin et al., 2014) images with associated sentence captions. We show that such knowledge transfer significantly improves performance on the video task. Our approach is inspired by recent breakthroughs reported by several research groups in image-to-text generation, in particular, the work by Donahue et al. (2014). They applied a version of their model to video-to-text generation, but stopped short of proposing an end-to-end single network, using an intermediate role representation instead. Also, they showed results only on the narrow domain of cooking videos with a small set of pre-defined objects and actors. Inspired by their approach, we utilize a Long-Short Term Memory (LSTM) recurrent neural network (Hochreiter and Schmidhuber, 1997) to model sequence dynamics, but connect it directly to a deep convolutional neural network to process incoming video frames, avoiding supervised intermediate representations altogether. This model is similar to their image-to-text model, but we adapt it for video sequences. Our proposed approach has several important advantages over existing video description work. The
LSTM model, which has recently achieved state-ofthe-art results on machine translation tasks (French and English (Sutskever et al., 2014)), effectively models the sequence generation task without requiring the use of fixed sentence templates as in previous
work (Guadarrama et al., 2013). Pre-training on image and text data naturally exploits related data to supplement the limited amount of descriptive video currently available. Finally, the deep convnet, the winner of the ILSVRC2012 (Russakovsky et al., 2014) image classification competition, provides a strong visual representation of objects, actions and scenes depicted in the video.
Our main contributions are as follows:
• We present the first end-to-end deep model for video-to-text generation that simultaneously learns a latent “meaning” state, and a fluent grammatical model of the associated language.
• We leverage still image classification and caption data and transfer deep networks learned on such data to the video domain.
• We provide a detailed evaluation of our model on the popular YouTube corpus (Chen and Dolan, 2011) and demonstrate a significant improvement over the state of the art.

### Related Work



### (中文)
### 摘要
&ensp;&ensp;长期以来，解决视觉符号接地问题一直是人工智能的一个目标。随着最近深度学习在静态图像的自然语言接地方面取得突破，该领域似乎正朝着这一目标迈进。在本文中，我们建议使用具有卷积和递归结构的统一深度神经网络将视频直接翻译成句子。描述视频的数据集很少，而且大多数现有方法都应用于可能词汇量较小的玩具领域。通过从 120 多万张带有类别标签的图像和 10 多万张带有标题的图像中转移知识，我们的方法能够为具有大量词汇的开放域视频创建句子描述。我们使用语言生成指标、主语、动词和宾语预测准确率以及人工评估，将我们的方法与最近的工作进行了比较。

### 介绍
&ensp;&ensp;对于大多数人来说，观看一段简短的视频并（用语言）描述所发生的事情是一件很容易的事情。对于机器来说，从视频像素中提取意义并生成自然语言是一个非常复杂的问题。人们已经提出了针对具有一小部分已知动作和对象的狭窄领域的解决方案，例如（Barbu 等人，2012 年；Rohrbach 等人，2013 年），但是为 YouTube 领域（图 1）等 "野生 "视频生成描述仍然是一个公开的挑战。开放域视频描述之所以难以取得进展，部分原因在于词汇量庞大，而由视频和相关描述句子组成的训练数据非常有限。另一个严重障碍是缺乏能捕捉帧序列和相应词序列的联合依赖关系的丰富模型。以前的工作通过检测一组固定的语义角色（如主语、动词和宾语）来简化问题（Guadarrama 等人，2013 年；Thomason 等人，2014 年），作为中间表示。这种固定的表示法对于庞大的词汇量来说是有问题的，而且还会导致过于简化的僵硬句子模板，无法模拟自然语言的复杂结构。在本文中，我们提出用一个深度神经网络将视频像素转换为自然语言。深度神经网络可以学习强大的特征（Donahue 等人，2013 年；Zeiler 和 Fergus，2014 年），但需要大量有监督的训练数据。我们通过从辅助任务中转移知识来解决这个问题。视频的每一帧都由一个卷积（空间不变）网络建模，该网络在 120 多万张带有类别标签的图像上进行了预先训练（Krizhevsky 等人，2012 年）。单词的意义状态和序列则由一个在 10 万多张带有相关句子标题的 Flickr（Hodosh 和 Hockenmaier，2014 年）和 COCO（Lin 等人，2014 年）图片上预先训练过的卷积（时间不变）深度网络建模。我们的研究表明，这种知识转移能显著提高视频任务的性能。我们的方法受到了几个研究小组最近在图像到文本生成方面取得的突破性进展的启发，特别是 Donahue 等人（2014 年）的工作。他们将其模型的一个版本应用于视频到文本的生成，但没有提出端到端单一网络，而是使用了中间角色表示。此外，他们仅在烹饪视频这一狭窄领域展示了结果，该视频包含一小部分预定义的对象和演员。受他们的方法启发，我们利用长短期记忆（LSTM）递归神经网络（Hochreiter 和 Schmidhuber，1997 年）来建立序列动态模型，但将其直接连接到深度卷积神经网络来处理输入的视频帧，从而完全避免了监督中间表征。这一模型与他们的图像到文本模型类似，但我们对其进行了调整，使其适用于视频序列。与现有的视频描述工作相比，我们提出的方法有几个重要优势。我们的
LSTM 模型最近在机器翻译任务（法语和英语（Sutskever et al.
Guadarrama 等人，2013 年）。在图像和文本数据上进行预训练，自然可以利用相关数据来补充目前有限的描述性视频。最后，作为 ILSVRC2012（Russakovsky 等人，2014 年）图像分类竞赛的优胜者，深度 convnet 为视频中描述的物体、动作和场景提供了强大的可视化表示。
我们的主要贡献如下：
- 我们提出了首个用于视频到文本生成的端到端深度模型，该模型可同时学习潜在的 "意义 "状态和相关语言的流畅语法模型。
- 我们利用静态图像分类和标题数据，将在此类数据上学习到的深度网络移植到视频领域。
- 我们在流行的 YouTube 语料库（Chen 和 Dolan，2011 年）上对我们的模型进行了详细评估，结果表明我们的模型比目前的技术水平有了显著提高。